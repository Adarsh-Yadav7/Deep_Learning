# What is Deep Learning?
# Deep Learning is a subset of Machine Learning that uses artificial neural networks to process and 
# learn from large amounts of data. It is inspired by the structure and function of the human brain, 
# consisting of multiple layers of neurons that extract and learn complex patterns from data.
#  Deep Learning is widely used in tasks such as image recognition, natural language processing (NLP), speech recognition, and more.

# Types of Deep Learning Models
# Feedforward Neural Network (FNN)

# The simplest type of neural network where data moves in one direction from input to output.
# Used in basic classification and regression tasks.
# Convolutional Neural Network (CNN)

# Specialized for image processing.
# Uses convolutional layers to detect spatial features like edges and textures.
# Applications: Image recognition, object detection, medical image analysis.
# Recurrent Neural Network (RNN)

# Designed for sequential data processing.
# Has loops to retain memory of previous inputs.
# Applications: Speech recognition, language modeling, time-series forecasting.
# Long Short-Term Memory (LSTM)

# A type of RNN that solves the vanishing gradient problem by using memory cells.
# Can retain long-term dependencies in sequences.
# Applications: Text generation, machine translation, stock market prediction.
# Gated Recurrent Unit (GRU)

# A simplified version of LSTM with fewer parameters.
# Faster and performs well on sequential data.
# Applications: Similar to LSTMs but preferred when computational efficiency is needed.
# Autoencoders

# Used for unsupervised learning tasks such as dimensionality reduction and anomaly detection.
# Consists of an encoder that compresses data and a decoder that reconstructs it.
# Applications: Data compression, noise reduction, feature learning.
# Generative Adversarial Networks (GANs)

# Consists of a generator and a discriminator that compete against each other.
# Used to generate new data that mimics real-world data.
# Applications: Image synthesis, deepfake generation, style transfer.
# Transformer Models

# Based on self-attention mechanisms, allowing parallel processing of input data.
# Revolutionized NLP with models like BERT and GPT.
# Applications: Machine translation, text generation, chatbots.
# Self-Organizing Maps (SOMs)

# Unsupervised learning technique that helps visualize and cluster high-dimensional data.
# Applications: Market segmentation, pattern recognition.
# Restricted Boltzmann Machine (RBM) & Deep Belief Networks (DBN)

# Probabilistic models used in feature learning and pre-training for deep networks.
# Applications: Collaborative filtering, dimensionality reduction.